{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Project 1\n",
    "\n",
    "### Project description:\n",
    "- Please read the Data Set Information section to learn about this dataset. \n",
    "- Data description is also provided for thi dataset.\n",
    "- Read data into Jupyter notebook, use pandas to import data into a data frame\n",
    "- Preprocess data: Explore data, check for missing data and apply data scaling. Justify the type of scaling used.\n",
    "\n",
    "### Regression Task:\n",
    "- Apply all the regression models you've learned so far. If your model has a scaling parameter(s) use Grid Search to find the best scaling parameter. Use plots and graphs to help you get a better glimpse of the results. \n",
    "- Then use cross validation to find average training and testing score. \n",
    "- Your submission should have at least the following regression models: KNN repressor, linear regression, Ridge, Lasso, polynomial regression, SVM both simple and with kernels. \n",
    "- Finally find the best regressor for this dataset and train your model on the entire dataset using the best parameters and predict buzz for the test_set.\n",
    "\n",
    "### Classification task:\n",
    "- Decide aboute a good evaluation strategy and justify your choice.\n",
    "- Find best parameters for following classification models: KNN classifcation, Logistic Regression, Linear Supprt Vector Machine, Kerenilzed Support Vector Machine, Decision Tree. \n",
    "- Which model gives the best results?\n",
    "\n",
    "### Deliverables:\n",
    "- Submit IPython notebook. Use markdown to provide an inline comments for this project.\n",
    "- Submit only one notebook. Before submitting, make sure everything runs as expected. To check that, restart the kernel (in the menubar, select Kernel > Restart) and then run all cells (in the menubar, select Cell > Run All).\n",
    "- Visualization encouraged. \n",
    "\n",
    "### Questions regarding project:\n",
    "- Post your queries related to project on discussion board on e-learning. There is high possibility that your classmate has also faced the same problem and knows the solution. This is an effort to encourage collaborative learning and also making all the information available to everyone. We will also answer queries there. We will not be answering any project related queries through mail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Set Information:\n",
    "This dataset is taken from a research explained here. \n",
    "\n",
    "The goal of the research is to help the auditors by building a classification model that can predict the fraudulent firm on the basis the present and historical risk factors. The information about the sectors and the counts of firms are listed respectively as Irrigation (114), Public Health (77), Buildings and Roads (82), Forest (70), Corporate (47), Animal Husbandry (95), Communication (1), Electrical (4), Land (5), Science and Technology (3), Tourism (1), Fisheries (41), Industries (37), Agriculture (200).\n",
    "\n",
    "There are two csv files to present data. Please merge these two datasets into one dataframe. All the steps should be done in Python. Please don't make any changes in csv files. Consider ``Audit_Risk`` as target columns for regression tasks, and ``Risk`` as the target column for classification tasks. \n",
    "\n",
    "### Attribute Information:\n",
    "Many risk factors are examined from various areas like past records of audit office, audit-paras, environmental conditions reports, firm reputation summary, on-going issues report, profit-value records, loss-value records, follow-up reports etc. After in-depth interview with the auditors, important risk factors are evaluated and their probability of existence is calculated from the present and past records.\n",
    "\n",
    "\n",
    "### Relevant Papers:\n",
    "Hooda, Nishtha, Seema Bawa, and Prashant Singh Rana. 'Fraudulent Firm Classification: A Case Study of an External Audit.' Applied Artificial Intelligence 32.1 (2018): 48-64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-8688c0ab9644>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-8688c0ab9644>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    pip install --upgrade scikit-learn\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('Audit_Risk.csv')\n",
    "data2= pd.read_csv('trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sector_score', 'LOCATION_ID', 'PARA_A', 'PARA_B', 'TOTAL', 'numbers', 'Money_Value', 'History', 'Score', 'Risk']\n"
     ]
    }
   ],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "print(intersection(list(data2.columns),list(data.columns)))\n",
    "common_cols=intersection(list(data2.columns),list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Money_Value'].median()\n",
    "data['Money_Value'].fillna((data['Money_Value'].median()), inplace=True)\n",
    "data2['Money_Value'].median()\n",
    "data2['Money_Value'].fillna((data2['Money_Value'].median()), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that there are no null values, I will search for a possible key to merge the tables on by creating a string of all the common columns to see if they can uniquely identify each row of their tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-181"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data['Risk'])-sum(data2['Risk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-be978b41fc51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Merging is performed and upon check it is found that the merge seems to have gone well, judging from the shape and head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(data, data2, how = 'inner' , on = 'LOCATION_ID' , left_index = True , right_index = True)\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "try :\n",
    "    common_cols.remove('LOCATION_ID')\n",
    "except ValueError:\n",
    "    print('Already removed key from common columns')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Defining functions to find(check_redundant) and remove(kill_redundant) columns that are present in both data frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_redundant(df,i):\n",
    "    x=i+'_x'\n",
    "    y=i+'_y'\n",
    "    if not ((df[x]==df[y]).sum()==len(df)):\n",
    "        return False\n",
    "    return True\n",
    "def kill_redundant(df,i):\n",
    "    x=i+'_x'\n",
    "    y=i+'_y'\n",
    "    df[i]=df[x]\n",
    "    col = [x,y]\n",
    "    df.drop(col, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using those functions to make the  merged data frame free from excess columns(redundant data) created from the merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in common_cols:\n",
    "    if check_redundant(df,i):\n",
    "        kill_redundant(df,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we check if the dataframe is good to go before exploring the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now I will check for the presence of null values and decide how to deal with them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "\n",
    "y=df['Risk_x']\n",
    "cols=list(df.columns)\n",
    "cols.remove('Risk_x')\n",
    "cols.remove('Risk_y')\n",
    "X=df[cols]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "train_score_array = []\n",
    "test_score_array = []\n",
    "\n",
    "for k in range(1,20):\n",
    "    knn = KNeighborsClassifier(k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_score_array.append(knn.score(X_train, y_train))\n",
    "    test_score_array.append(knn.score(X_test, y_test))\n",
    "    \n",
    "x_axis = range(1,20)\n",
    "%matplotlib inline\n",
    "plt.plot(x_axis, train_score_array, label = 'Train Score', c = 'g')\n",
    "plt.plot(x_axis, test_score_array, label = 'Test Score', c='b')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
